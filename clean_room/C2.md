# Database: C2

## M·ª•c l·ª•c (4 b√†i)
- [C2_Week1](#c2_week1)
- [C2_Week2](#c2_week2)
- [C2_Week3](#c2_week3)
- [C2_Week4](#c2_week4)

---

## <a name="c2_week1"></a>C2_Week1


### Neurons and the brain


    Ph·∫ßn n√†y gi·ªõi thi·ªáu s∆° qua v·ªÅ c√°c thu·∫≠t to√°n c·ªë b·∫Øt ch∆∞·ªõc c√°ch m·ªôt chi·∫øc n√£o ho·∫°t ƒë·ªông, m·ªü ƒë·∫ßu v√†o c√°c nƒÉm 80s t·ªõi 90s, nh∆∞ng sau ƒë√£ b·ªõt th·ªãnh h√¨nh v√†o cu·ªëi c√°c nƒÉm 90s ·∫•y.


    ‚Üí V√†o kho·∫£ng 2005 ƒë√£ quay l·∫°i, v·ªõi c√°c ·ª©ng d·ª•ng nh∆∞ nh·∫≠n bi·∫øt gi·ªçng n√≥i, h√¨nh ·∫£nh v√† k√Ω t·ª±.


    ![H√¨nh ·∫£nh Neurons ·ªü trong n√£o](./images/H√¨nh ·∫£nh Neurons ·ªü trong n√£o_2e513.png)


    Khi b·∫Øt ch∆∞·ªõc m·ªôt neuron trong ch∆∞∆°ng tr√¨nh m√°y t√≠nh, ta c√≥ m·ªôt Simplified Mathematical model of a Neuron:

    - Nh·∫≠n v√†o input ‚Üí t√≠nh to√°n ‚Üí Output

    ![image](./images/image_2e513.png)

    - L√Ω do m√¨nh s·ª≠ d·ª•ng m√¥ h√¨nh n√†y l√† v·ªõi s·ª± ph√°t tri·ªÉn c·ªßa Big Data, m·ªôt h·ªá th·ªëng Large Neural Network s·∫Ω ƒëem t·ªõi Performance t·ªët nh·∫•t c√≥ th·ªÉ v·ªõi l∆∞·ª£ng Data ·∫•y.

    ![image](./images/image_2e513.png)


---


### Demand Prediction


    Ph·∫ßn n√†y vi·∫øt v·ªÅ m·ªôt b√†i to√°n d·ª± ƒëo√°n nhu c·∫ßu mua s·∫Øp c·ªßa m·ªôt s·∫£n ph·∫©m n√†o ƒë√≥. 


    V√≠ d·ª•, ƒë·ªÉ ƒë∆∞a ra ƒë∆∞·ª£c d·ª± ƒëo√°n ·∫•y m√¨nh s·∫Ω th·ª≠ t√≠nh to√°n xem t·ª∑ l·ªá c·ªßa vi·ªác s·∫£n ph·∫©m c√≥ ph·∫£i l√† m·ªôt Top-Seller hay kh√¥ng? Ta s·∫Ω s·ª≠ d·ª•ng m·ªôt h√†m Sigmoid, g·ªçi l√† m·ªôt Activation.


    
$
f(z)=\frac{1}{1+e^{-z}}
    $
- Vi·ªác s·ª≠ d·ª•ng m·ªôt Neuron s·∫Ω ƒë∆∞·ª£c minh ho·∫° nh∆∞ n√†y:

    ![image](./images/image_2e513.png)


    V·ªõi m·ªôt h·ªá th·ªëng c√°c Neurons nhi·ªÅu v√† ƒë·∫ßy ƒë·ªß h∆°n, ta c√≥ th·ªÉ s·ª≠ d·ª•ng c√°c y·∫øu t·ªë ƒë·∫ßu v√†o nh∆∞: gi√° c·∫£, gi√° ship, marketing, ch·∫•t li·ªáu, ƒë∆∞a qua m·ªôt ‚Äúl·ªõp‚Äù Neurons ƒë·ªÉ t·∫°o ra c√°c th√¥ng s·ªë nh∆∞: affordability, awareness, quality. Cu·ªëi c√πng ƒë∆∞a v√†o m·ªôt layer kh√°c ƒë·ªÉ t·∫°o ra ‚Äút·ª∑ l·ªá l√† m·ªôt Top-Seller‚Äù


    ![image](./images/image_2e513.png)

    - Layer cu·ªëi c√πng ƒë∆∞·ª£c g·ªçi l√† Output Layer
    - M·ªôt Layer c√≥ th·ªÉ c√≥ nhi·ªÅu Neurons (layer 1 c√≥ 3, layer 2 ch·ªâ c√≥ 1)
    - C√°c Layer ·ªü gi·ªØa ƒë∆∞·ª£c g·ªçi l√† ‚ÄúHidden Layers‚Äù
    - Activation l√† m·ªôt h√†m to√°n h·ªçc (trong tr∆∞·ªùng h·ª£p n√†y l√† Sigmoid) ƒë·ªÉ ƒë∆∞a ra output c·ªßa Neuron.

    ‚Üí M√¨nh c√≥ th·ªÉ hi·ªÉu t·ª´ 4 output v·ª´a r·ªìi, vi·ªác m√¨nh ƒë∆∞a qua l·ªõp Neuron ƒë·∫ßu ti√™n ƒë·ªÉ ƒë∆∞a ra c√°c th√¥ng s·ªë m·ªõi ·∫•y, t∆∞∆°ng ƒë∆∞∆°ng v·ªõi **Feature Engineering** ·ªü Course 1. 


    C√≥ th·ªÉ c√≥ nhi·ªÅu hidden layer nh∆∞ n√†y:


    ![image](./images/image_2e513.png)


---


### Examples


    Trong ph·∫ßn n√†y s·∫Ω ƒë∆∞a ra 2 ·ª©ng d·ª•ng c·ªßa m·ªôt Neural Networks. Nh∆∞ng s·∫Ω ch·ªâ gi·∫£i th√≠ch v√≠ d·ª• ƒë·∫ßu ti√™n v·ªÅ nh·∫≠n di·ªán khu√¥n m·∫∑t, v√≠ d·ª• th·ª© 2 v·ªÅ nh·∫≠n di·ªán √¥ t√¥ t∆∞∆°ng t·ª±.

    1. ·ª®ng d·ª•ng trong nh·∫≠n di·ªán khu√¥n m·∫∑t:
    - T∆∞·ªüng t∆∞·ª£ng m√¨nh c√≥ m·ªôt b·ª©c ·∫£nh, v·ªõi k√≠ch th∆∞·ªõc 1000 x 1000 pixels, khi m√¨nh l·∫•y b·ª©c ·∫£nh l√†m ƒë·∫ßu v√†o c·ªßa Model, b·ª©c ·∫£nh s·∫Ω th√†nh m·ªôt chi·∫øc b·∫£ng ch·ª©a m·ª©c ƒë·ªô s√°ng c·ªßa t·ª´ng pixel trong ·∫£nh.

    ![image](./images/image_2e513.png)

    - ƒê·ªÉ vi·∫øt d·ªÖ h∆°n m√¨nh s·∫Ω ƒë·∫°i di·ªÖn th√¥ng tin n√†y b·∫±ng m·ªôt vector c√≥ 1000x1000 = 1 tri·ªáu ph·∫ßn t·ª≠:

    ![image](./images/image_2e513.png)

    - ƒê√¢y l√† slide mi√™u t·∫£ ƒëi·ªÅu g√¨ s·∫Ω x·∫£y ra khi m√¨nh input th√¥ng tin n√†y v√†o m·ªôt Neural network:

    ![image](./images/image_2e513.png)

    - Layer 1:  c√°c ƒë∆∞·ªùng v·∫Ω, ho·∫∑c c·∫°nh/ n√©t, c√°c v√πng s√°ng ho·∫∑c t·ªëi g·∫ßn v·ªõi nhau.
    - Layer 2: Gh√©p c√°c ƒë∆∞·ªùng n√©t/ c·∫°nh ·∫•y th√†nh t·ªï h·ª£p c√°c h√¨nh ph·ª©c t·∫°p h∆°n nh∆∞ g√≥c m≈©i, v√†nh t√†i (ki·ªÉu c√°c b·ªô ph·∫≠n nh·ªè c·ªßa m·∫∑t)
    - Layer 3: K·∫øt h·ª£p c√°c chi ti·∫øt ·∫•y l·∫°i th√†nh c√°c b·ªô ph·∫≠n √µ h∆°n nh∆∞ m·∫Øt, m≈©i, mi·ªáng‚Ä¶
    - Output layer: t·ª∑ l·ªá ho·∫∑c x√°c su·∫•t b·ª©c ·∫£nh ƒë·∫•y l√† ng∆∞·ªùi ‚ÄúXYZ‚Äù (t·ª´ 0 - 1 v√¨ h√†m Sigmoid)
    1. Nh·∫≠n di·ªán/ Ph√¢n lo·∫°i xe √¥ t√¥ (quy tr√¨nh gi·ªëng nh∆∞ tr√™n)

    ![image](./images/image_2e513.png)


---


### Neural Network Layer


    ![image](./images/image_2e513.png)


    Sau n√†y s·∫Ω s·ª≠ d·ª•ng m·ªôt s·ªë k√Ω hi·ªáu nh∆∞ sau:

    - M·ªôt activation vector(m·ªôt tr·ªçng s·ªë, bias, ‚Ä¶) ·ªü m·ªôt t·∫ßng x b·∫•t k·ª≥, s·∫Ω c√≥ k√Ω hi·ªáu nh∆∞ n√†y:

    
$
\vec{a}^{[x]}
    $
- Do trong m·ªôt s·ªë layer, s·∫Ω c√≥ nhi·ªÅu gi√° tr·ªã tr·ªçng s·ªë t∆∞∆°ng ·ª©ng v·ªõi t·ª´ng Neuron trong l·ªõp ·∫•y, ta s·∫Ω vi·∫øt:

    
$
w_{1}^{[1]}
    $
‚Üí Con s·ªë ƒë·∫°i di·ªán l·ªõp n·∫±m ·ªü tr√™n, s·ªë th·ª© t·ª± n·∫±m ·ªü d∆∞·ªõi (b√™n tr√™n l√† weight s·ªë 1, ·ªü layer 1)

    - C√¥ng th·ª©c activation c·ªßa c√°c Neuron(1, 2, 3) ƒë∆∞·ª£c vi·∫øt d∆∞·ªõi d·∫°ng:

    
$
a_{1}^{[1]} = g\!\left(w_{1}^{[1]}\cdot \vec{x} + b_{1}^{[1]}\right)
    $
> üí° > T∆∞∆°ng t·ª± v·ªõi 2 neuron c√≤n l·∫°i

    - Sau khi ch·∫°y xong l·ªõp th·ª© nh·∫•t m√¨nh s·∫Ω c√≥ ƒë∆∞·ª£c m·ªôt vector nh∆∞:

    
$
\begin{bmatrix}a_{1}^{[1]}\\a_{2}^{[1]}\\a_{3}^{[1]}\end{bmatrix}
    $
- L∆∞u √Ω r·∫±ng do ƒë√¢y l√† Layer 1 (layer ngay sau input) n√™n m√¨nh ƒëang nh√¢n t√≠ch v√¥ h∆∞·ªõng c·ªßa tr·ªçng s·ªë v·ªõi Input vector x.

    ‚Üí ƒê·ªëi v·ªõi c√¥ng th·ª©c activation t·ª´ layer 2 tr·ªü ƒëi ph·∫£i thay vector x th√†nh vector c·ªßa l·ªõp tr∆∞·ªõc ƒë√≥:


    
$
a_{1}^{[2]} = g\!\left( w_{1}^{[2]} \cdot \vec{a}^{[1]} + b_{1}^{[2]} \right)
    $
> üí° Neuron s·ªë 1 c·ªßa l·ªõp s·ªë 2

    - Sau khi m√¨nh ch·∫°y xong t·ªõi output layer th√¨ m√¨nh c√≥ th·ªÉ so s√°nh v·ªõi m·ªôt decision boundary (ch·∫≥ng h·∫°n 0.5) ƒë·ªÉ ƒë√°nh gi√° xem y=1 hay y=0.

        ![image](./images/image_2e513.png)


---


### More Complex Neural Networks


    C√πng v·ªõi c√°c c√°ch k√Ω hi·ªáu nh∆∞ tr√™n, m·ªôt Neural Network ph·ª©c t·∫°p h∆°n s·∫Ω nh√¨n nh∆∞ n√†y:


    ![image](./images/image_2e513.png)


    Do k√Ω hi·ªáu kh√° nhi·ªÅu n√™n m√¨nh c√≥ th·ªÉ ƒë∆∞a ra m·ªôt c√¥ng th·ª©c t·ªïng qu√° cho Output c·ªßa m·ªôt Neuron b·∫•t k·ª≥:


    
$
a_{j}^{[l]} = g\!\left(\vec{w}_{j}^{[l]}\cdot \vec{a}^{[l-1]} + b_{j}^{[l]}\right)
    $
Note:


    ![image](./images/image_2e513.png)


---


### Inference: Making Predictions (forward propagation)


    Ch·∫≥ng h·∫°n v·ªõi m·ªôt b·ª©c tranh pixel 8x8 ƒë∆∞·ª£c tr·∫£i th·∫≥ng th√†nh vector c√≥ 64 ph·∫ßn t·ª≠ ƒë·ªÉ ƒë∆∞a v√†o Neural Network, m·ªói ph·∫ßn t·ª≠ n·∫±m trong kho·∫£ng [0, 255] bi·ªÉu th·ªã m·ª©c ƒë·ªô s√°ng. V·ªõi 0 l√† m√†u ƒëen, 255 l√† m√†u tr·∫Øng.


    ![image](./images/image_2e513.png)

    - V√† Neural Network c√≥ c·∫•u tr√∫c:

        ![image](./images/image_2e513.png)

    - layer 1: **25 units**
    - layer 2: **15 units**
    - layer 3 (output layer): **1 unit**

    Activation vector c·ªßa l·ªõp s·ªë 1 s·∫Ω c√≥ 25 ph·∫ßn t·ª≠, m·ªói ph·∫ßn t·ª≠ l√† output c·ªßa h√†m sigmoid: (t∆∞∆°ng t·ª± v·ªõi c√°c l·ªõp kh√°c)


    
$
\vec{a}^{[1]}=\begin{bmatrix}g(\vec{w}_{1}^{[1]}\cdot\vec{x}+b_{1}^{[1]})\\\vdots\\g(\vec{w}_{25}^{[1]}\cdot\vec{x}+b_{25}^{[1]})\end{bmatrix}
    $
V·ªõi s·ª± l·∫∑p ƒëi l·∫∑p l·∫°i qua t·ª´ng l·ªõp:


    
$
\vec{x}\rightarrow \vec{a}^{[1]}\rightarrow \vec{a}^{[2]}\rightarrow a^{[3]}
    $
Ta g·ªçi ƒë√¢y l√† forward propagation, qu√° tr√¨nh ƒëi t·ª´ tr√°i sang ph·∫£i c·ªßa m·ªôt m·∫°ng. T·ª´ ƒë√≥ c√≥ th·ªÉ quy·∫øt ƒë·ªãnh output xem l√† 1 ho·∫∑c 0.


    
$
\hat{y} =\begin{cases}1, & a^{[3]} \ge 0.5\\0, & a^{[3]} < 0.5\end{cases}
    $


---

## <a name="c2_week2"></a>C2_Week2



---

## <a name="c2_week3"></a>C2_Week3



---

## <a name="c2_week4"></a>C2_Week4



---
